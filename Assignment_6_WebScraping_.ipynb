{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrheavyman/Price_DTSC3020_Fall2025/blob/main/Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af85116-5b20-4941-a761-f66a77644de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "#Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0df16c4-2aec-444d-b631-940acfb9c38b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416aa214-176d-4ad1-be48-680738d2594a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1163708185.py:18: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "URL = \"https://www.iban.com/country-codes\"\n",
        "\n",
        "def _flatten_columns(cols: pd.Index) -> List[str]:\n",
        "  if isinstance(cols, pd.MultiIndex):\n",
        "    return [\n",
        "        \" \".join([str(part) for part in tup if part is not None and str(part) != \"nan\"]).strip()\n",
        "        for tup in cols.tolist()\n",
        "    ]\n",
        "  else:\n",
        "    return [str(c).strip() for c in cols.tolist()]\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    tables = pd.read_html(html)\n",
        "    if not tables:\n",
        "      raise ValueError(\"No tables found in provided HTML\")\n",
        "    for t in tables:\n",
        "      if t.shape[1] >= 3:\n",
        "        t = t.copy()\n",
        "        t.columns = _flatten_columns(t.columns)\n",
        "        t = t.loc[:, [c for c in t.columns if c and c.strip()]]\n",
        "        return t\n",
        "\n",
        "resp = requests.get(URL, timeout=30)\n",
        "resp.raise_for_status()\n",
        "html = resp.text\n",
        "\n",
        "df_raw = q1_read_table(html)\n",
        "\n",
        "assert 'df_raw' in globals(), \"df_raw not found in previous step.\"\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    col_map = {}\n",
        "\n",
        "    for c in df.columns:\n",
        "      c_norm = c.strip().lower()\n",
        "      if c_norm in (\"country\", \"country/territory\", \"country or area\"):\n",
        "        col_map[c] = \"Country\"\n",
        "      elif \"alpha-2\" in c_norm or \"alpha 2\" in c_norm:\n",
        "        col_map[c] = \"Alpha-2\"\n",
        "      elif \"alpha-3\" in c_norm or \"alpha 3\" in c_norm:\n",
        "        col_map[c] = \"Alpha-3\"\n",
        "      elif \"numeric\" in c_norm or \"num\" in c_norm or \"iso 3166-1 numeric\" in c_norm:\n",
        "        col_map[c] = \"Numeric\"\n",
        "      else:\n",
        "        col_map[c] = c.strip()\n",
        "    df.rename(columns=col_map, inplace=True)\n",
        "\n",
        "    # Fix typo: df.coulmns to df.columns\n",
        "    for c in df.columns:\n",
        "      if pd.api.types.is_object_dtype(df[c]):\n",
        "        df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "    if \"Alpha-2\" in df.columns:\n",
        "      df[\"Alpha-2\"] = df[\"Alpha-2\"].str.upper()\n",
        "    if \"Alpha-3\" in df.columns:\n",
        "      df[\"Alpha-3\"] = df[\"Alpha-3\"].str.upper()\n",
        "\n",
        "    if \"Numeric\" in df.columns:\n",
        "      df[\"Numeric\"] = pd.to_numeric(df[\"Numeric\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    required_cols_to_check = [c for c in [\"Country\", \"Alpha-2\", \"Alpha-3\", \"Numeric\"] if c in df.columns]\n",
        "\n",
        "    if required_cols_to_check:\n",
        "      df = df.dropna(subset=required_cols_to_check)\n",
        "      for c in required_cols_to_check:\n",
        "        if pd.api.types.is_object_dtype(df[c]):\n",
        "          df = df[df[c].astype(str).str.strip() != \"\"]\n",
        "\n",
        "    # Select only the explicitly requested columns\n",
        "    requested_final_cols = [c for c in [\"Country\", \"Alpha-2\", \"Alpha-3\", \"Numeric\"] if c in df.columns]\n",
        "\n",
        "    return df[requested_final_cols]\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by Numeric desc and return Top-N. TODO: implement.\"\"\"\n",
        "    df = df.copy()\n",
        "    if \"Numeric\" in df.columns:\n",
        "      if \"Country\" in df.columns:\n",
        "        df = df.sort_values(by=[\"Numeric\", \"Country\"], ascending=[False, True])\n",
        "      else:\n",
        "        df = df.sort_values(by=[\"Numeric\"], ascending=[False])\n",
        "    return df.head(top).reset_index(drop=True)\n",
        "\n",
        "    readme_text = \"\"\"# Assignment 6, Question 1 — Web Scraping\n",
        "\n",
        "## URL\n",
        "https://www.iban.com/country-codes\n",
        "\n",
        "## Steps\n",
        "1.  **Fetch HTML**: The HTML content from the specified URL was fetched using `requests`.\n",
        "2.  **Extract Table**: The `q1_read_table` function used `pandas.read_html` to identify and extract the main country codes table. It then flattened the multi-level headers to single-level headers for easier processing.\n",
        "3.  **Clean Data**: The `q1_clean` function performed the following cleaning steps:\n",
        "    *   Standardized column names to `Country`, `Alpha-2`, `Alpha-3`, and `Numeric`.\n",
        "    *   Trimmed whitespace from all string columns.\n",
        "    *   Converted `Alpha-2` and `Alpha-3` codes to uppercase.\n",
        "    *   Converted the `Numeric` column to a nullable integer type, coercing non-numeric values to `NaN`.\n",
        "    *   Dropped rows where `Country`, `Alpha-2`, `Alpha-3`, or `Numeric` had missing values (after conversion).\n",
        "4.  **Save Output**: The cleaned DataFrame will be saved to `data_q1.csv`.\n",
        "5.  **Print Top-15**: The `q1_sort_top` function sorts the cleaned DataFrame by `Numeric` in descending order (and then by `Country` alphabetically for ties) and returns the top 15 entries, which will be printed to the console.\n",
        "\n",
        "## Limitation\n",
        "The `pandas.read_html` function is highly dependent on the HTML structure of the target page. If the website's layout changes significantly, especially the table's structure or its surrounding tags, the `q1_read_table` function might fail to correctly identify or parse the desired table, requiring updates to the parsing logic.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q2_skeleton_answer"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    items = []\n",
        "    for row in soup.select(\"tr.athing\"):\n",
        "\n",
        "        rank_text = row.select_one(\"span.rank\")\n",
        "        rank = rank_text.get_text(strip=True).split(\".\")[0] if rank_text else \"\"\n",
        "\n",
        "        titlelink = row.select_one(\"span.titleline a\")\n",
        "        title = titlelink.get_text(strip=True) if titlelink else \"\"\n",
        "        link = titlelink.get(\"href\", \"\") if titlelink else \"\"\n",
        "\n",
        "\n",
        "        sub = row.find_next_sibling(\"tr\")\n",
        "        subtext = sub.select_one(\".subtext\") if sub else None\n",
        "\n",
        "\n",
        "        points_el = subtext.select_one(\"span.score\") if subtext else None\n",
        "        points = points_el.get_text(strip=True) if points_el else \"\"\n",
        "\n",
        "\n",
        "        user_el = subtext.select_one(\"a.hnuser\") if subtext else None\n",
        "        user = user_el.get_text(strip=True) if user_el else \"\"\n",
        "\n",
        "        comments = \"\"\n",
        "        if subtext:\n",
        "            links = subtext.select(\"a\")\n",
        "            if links:\n",
        "                comments = links[-1].get_text(strip=True)\n",
        "\n",
        "        items.append({\n",
        "            \"rank\": rank,\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"points\": points,\n",
        "            \"comments\": comments,\n",
        "            \"user\": user\n",
        "        })\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton_answer"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}